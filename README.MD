# Books references

This is roughly the elements needed for each slide

## First Part - Linear Models

### 01 Introduction

- Mitchel - Chapter 1 : Introduction
  - 1.1 : Well-posed learning problems
  - 1.2 : Designing a learning system
  - 1.3 : Perspectives and issues in machine learning
- Norvig - Chapter 19 : Learning from examples (optional)
  - 19.1 : Forms of learning
  - 19.2 : Supervised learning
- Mitchel - Chapter 2 : Concept learning and general to specific ordering
  - 2.1 : A concept learning task
  - 2.3 : Concept learning as a search
  - 2.5 : Version spaces and the candidate-elimination algorithm
  - 2.6 : Remarks on version spaces and candidate-elimination (optional)
  - 2.7 : Inductive bias

### 02 Classification and evaluation

- Mitchel - Chapter 5 : Evaluating hypotesis
  - 5.1 : Motivation
  - 5.2 : Estimating hypotesis accuracy
  - 5.3 : Basics of Sampling theory
    - 5.3.4 : Estimators, bias and variance
  - 5.5 : Difference in error of two hypotesis
- Google developers - Classification:
  - [true vs false and positive vs negative](https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative)
  - [Accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy)
  - [Precision and recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)
  - [Roc curve and UAC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)

### 03 Decision trees

- Mitchel - Chapter 3 : Decision trees
  - 3.1 : Introduction
- Norvig - Chapter 19 : Learning Decision trees (optional)
  - 19.3.0 : Learning Decision trees
- Mitchel - Chapter 3
  - 3.2 : Decision Tree Representation
  - 3.3 : Appropriate problems for decision tree learning
- Norvig - Chapter 19 (optional)
  - 19.3.1 : Expressiveness of decision trees
  - 19.3.2 : Learning decision trees from examples
  - 19.3.3 : Choosing attributes to test
- Bishop - Chapter 1 (optional):
  - 1.6 : Information theory
- Mitchel - Chapter 3
  - 3.4 : Basic decision tree learning algorithm
  - 3.5 : Hypotesis space search in decision tree learning
  - 3.6 : Inductive bias in decision tree learning
  - 3.7 : Issues in decision learning
- Norvig - Chapter 19
  - 19.3.4 : Generalization and overfitting

### 04 Probability and Bayes networks

- Bishop - Chapter 1 : Introduction:
  - 1.2 : Probability theory
- Norvig chapter 12 : Quantifying uncertainty:
  - 12.1 : Acting under uncertainty
  - 12.2 : Basic probability notation
  - 12.3 : Inference using full joint distribution
  - 12.4 : Indipendence
  - 12.5 : Bayes' rule and its use
  - 12.6 : Naive Bayes' models
- Norvig - Chapter 13 : Probabilistic reasoning (optional)
  - 13.1 Representing knowlege in an uncertain domain
  - 13.2 The semantics of bayesan networks
  - 13.3 Exact inference in bayesan networks

### 05 Bayesian Learning

- Mitchel chapter 6 :

### 06 Probabilistic models for classification

- Bishop - Chapter 1 : Introduction
  - 1.5 : Decision theory
    - 1.5.1 : Minimizing the misclassifcation rate
    - 1.5.2 : Minimizing the expected loss
    - 1.5.3 : The Reject option
    - 1.5.4 : Inference and decision

- Bishop chapter 4:
  - 4.2 : Probabilistic generative models
  - 4.3 : Probabilsitic discriminative models:

### 07 Linear models for classification

- Bishop chapter 4:
  - 4.1 : Discriminant functions
- Mitchel chapter 4:
  - 4.1 : Introduction
  - 4.2 : Neural networks representations
  - 4.3 : Appropriate problems for neural network learning
  - 4.4 : Perceptrons
- Bishop chapter 7:
  - 7.1 : Maximum margin classifiers

### 08 Linear models for regression

- Bishop chapter 3:
  - 3.1 : Linear basis function models

### 09 Kernel Models

- Mitchel chapter 6
- Bishop chapter 7:
  - 7.1 Maximum margin classifiers
